The basic logic of our programFind and read each file; 
for each file, identify whether file is a directory or ‘true’ file. 
If directory, recursively parse through it (call the directory parser function), and do the same thing again; 
for each file inside the directory, identify whether file is a directory or file. 
Now, if at any point a file is a ‘true’ file, we open the file and tokenize the words. 
For each word, check whether the word is already in our hash map; 
if yes, if not, store it.
see if file name is already in the word’s file list.
if yes, increase occurrence count. 
If not, add file to list and make occurrence 1. 

After all files are read, it puts all hash nodes in a sorted list.
Complexity analysis
-See if token is already stored in hash map: average O(1), worst O(k) where k = number of hash nodes at that bucket
-See if file name is already represented for a given token: linear search, O(n)
-Adding a file to file list of a word’s bucket: quick sort at insertion time, always O(n log(n))
-Inserting all hash nodes into sorted list: worst O(n log(n)), n being number of unique tokens (it will never actually be this bad because n isn't a constant, but grows as the sorted list grows. First insertion would be O(1), then n=1. Next insertion n=2. And so on...)
-Parsing through all directories: open & close, 2 * O(1) for n files = O(n)